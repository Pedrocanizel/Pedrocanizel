..

s3 / Vídeo 32:

Serve para:

Backup, Armazenamento, Arquivamento de dados, Armazenamento de núvem hibrida, Hospedar aplicações, 
Hospedar mímia, data lake, Analytics, Delivery de software,  Site estático.

Nomes dos buckets devem ser globalmente únicos.

Buckets são definidos à nivel regional,

Parecem um serviço global, mas são criados em região.

Os objetos (arquivos) no bucket possuem uma chave, essa chave é o full path para esse objeto.
exemplo: s3://my-bucket/my_file.txt, s3://my-bucket/one_folder/my_file.txt

O tamanho máximo de um objeto é de 5tb,

Caso o arquivo possua mais de 5tb vc deve usar o "multi-part upload",

Os metadados do s3 são lista de chave/valor para identificar os objetos.

O objeto pode ter uma 'version ID' se o versionamento estiver habilitado, após desabilitar a opção de versionamento,
as versões dos seus objetos não são deletadas.


Segurança no s3 / Vídeo 34 :

Baseadas no usuário:

IAM Policies que dão as permissões de quais apis o usuário possue permissão,

Baseadas no recurso: 

Regras no bucket a partir do console do s3, que dão permissões através das contas.
ACL: Object acess control list, granularidade fina de permissões que pode ser desabilitado.
ACL Bucket acess control list: Controle de acesso à nivel do bucket

Então, quando uma pessoa pode acessar um objeto no s3? Se ela tiver permissão no IAM OU permissão no recurso
e não tiver nenhum tipo de proibição (DENY).

Encriptação: Objetos no s3 usam chaves de encriptação.

s3 Replicação / Vídeo 38:

CRR : Cross Region Replication
SRR : Same Region Replication

Para fazer replicação, primeiro devo habilitar o Versionamento dos buckets, tanto na região que eu estou, quanto
na região de destino caso eu vá fazer CRR.

Os buckets podem estar em diferentes contas da AWS,
A cópia é assíncrona.

É necessário dar permissões de leitura e escrita para o s3 para fazer a replicação.

Depois que você ativa replicação, apenas os objetos novos são replicados.

Caso você queira replicar os objetos que já existiam, vc deve usar o s3 Batch Replication.

Você pode habilitar a replicação de delete markers, nesse caso, apenas os delete markers são replicados, não acontece a deleção real do objeto


Não há encadeamento de replicações, ou seja, você replica apenas do bucket 1 para o bucket 2. Caso
o bucket 2 possua replicação com o bucket 3, ele não encadeia a replicação do 1, para o 2, para o 3 e assim sucessivamente.


s3 Storage classes / Vídeo 41:

Você pode mudar manualmente entre as classes de armazenamento ou de forma automática usando configurações de ciclo de vida.

Conceito de durabilidade: Quanto um objeto vai durar na nuvem - 99,9999999999%, 11 9's

A durabilidade é a mesma em todas das classes de armazenamento no s3.

Disponibilidade diz quão disponível um serviço é, isso varia de classe para classe de armazenamento.

s3 Standard -> Propósito geral:

- 99,99% de disponibilidade,
- Usado para acesso frequente aos dados,
- Baixa Latência e alto throughput,
- Sustain 2 concurrent facility failures?

Use cases: Big Data Analytics, mobile & gaming aplication, content distribution...

s3 infrequent acsees:

- Para dados que são menos frequentemente acessados mas requerem acesso rapido quando preciso,
- Menos custo do que s3 standard.

S3 standard-Infrequent Acess (s3 Standard-IA)
- 99,9% disponibilidae,
- Usado para disaster recovery backups

s3 one zone-infrequent acess(s3 One Zone-IA)
- 99,999999999999999% durabilidade em uma unica AZ mas o dado é destruído quando a AZ é destruída.
- 99,5% Disponibilidade.

Use cases: backups secundários de cópias do on-premise ou dado q vc pode recriar.

s3 Glacier Storage Classes:

- menores custos por armazenamento,
- Paga por armazenamento + acesso.

S3 glacier instant retrieval:
- Acesso em millisegundos, bom para dados acessados por trimestre
- Mínimo de 90 dias guardado.

s3 flacier flexible retrieval:
- Expedited (1 a 5 min), Standard (3 a 5hr), Bulk (5 a 12hr)-free,
- Mínimo de 180 dias armazenados.

s3 Inteligent-Tiering:

- Move objetos automaticamente entre as classes de armazenamento conforme uso.
- Não tem custo para acessar o dado.

A ordem de mudança dos objetos é:

Frequent acess tier (automatic) default:
Infrequent acess tier (automatic) objects not acessed for 30 days,
Archive instant access tier (automatic) objects not acessed for 90 days,
Archive acess tier (optional) configurable from 90 days to 700 + days
Deep archive access tiver (optional) config from 180 days to 700+ days.

Ver o diagrama de comparação no final do módulo.


s3 Lifecycle rules / Vídeo 43:

Transition action: 
- Configura objetos para serem movidos entre classes de armazenamento.

Expiration actions:
- Configura objetos para expirar, deletar após um tempo.


lifecycle rules / Vídeo 44:

- current version é a mais recente,


s3 event notifications / Vídeo 45:

Podem ser alvo das notificações:

SNS, 
SQS,
Lambda function,
Amazon eventbridge que pode levar esses eventos para + 18 ferramentas da aws.


s3 Performance / Vídeo 47:

Podemos ter até 3500 put/copy/post/delete requests/s
ou
5500 get/head requests/s por prefixo em um bucket

Multi-part upload (ajuda paralelizar uploads para potencializar a largura de banda):
- Recomendado para arquivos > 100mb,
- Deve ser usado para arquivos > 5gb,

s3 transfer acceleration : 
- aumenta a velocidade de transferencia, tanto para download quanto para upload
-  compativel com multi part upload

s3 byte-range fetches:

- Paraleliza gets por requisitar especificos ranges de bytes,
- Melhor resiliencia em caso de falhas.
- Pode buscar apenas uma determinada feixa de bytes.


s3 Encryption / Vídeo 49:

- SSE - Server side encryption,
- CSE - Client side encryption,

É importante entender todos os tipos de encriptação para o exame.

SSE-S3 encryption:
- A chave usada para encruptação é lidada e gerenciada e possuída pela própria AWS, vc nunca tem acesso à essa chave,
- O tipo de encriptação é o AES-256
- Deve conter um header "..........."AES256"
- Habilitada por padrão para novos buckets e novos objetos,

SSE-KMS encryption:
- Chaves de encriptação lidadas e gerenciadas pela AWS KMS(Key management Service),
- Vantagens: Controle do usuário + chave para auditoria usando o Cloudtrail,
- Objeto é encriptado pelo lado do servidor,
- Deve ter um header "................"aws:kms",
- Possui algumas limitações, você precisa acessar o KMS servicem decriptar, 

SSE-C encryption:
- Server-side encryption usando chaves gerenciadas pelo client fora da AWS,
- s3 não armazena a chave de encriptação que vc provê.
- HTTPS devem ser usados,
- Chave de encriptação deve ser provida em HTTP headers para cada requisição HTTP feita.

Client-Side encryption:
- Usa bibliotecas para encriptação como amazon s3 client-side encryption library,
- Cliente deve encriptar o dado antes de subir para o s3,
- Clientes devem decriptar o dado depois que tiram ele do s3, 
- Cliente gerencia as chaves e o ciclo de encriptação.

Encriptação em trânsito/em vôo tbm chamada de SSL/TLS:
s3 expõe 2 endpoints:
- HTTP endpoint que não é encriptado,
- HTTPS Endpoint que é encriptado em Vôo (recomendado),

HTTPS é mandatório para SSE-C,

Como forçar encriptação em trânsito?
- Usar política no Bucket.
Você nega getoperation colocando False para awssecuretransport, dessa forma o http é negado, apenas o https é permitido.


DynamoDb overview / Vídeo 54:

- NoSQL não suportam joins,
- Todo dado necessário está presente em uma linha,
- Não performam agregações,
- Escala muito bem horizontalmente,


DinamoDb é: 
- Altamente disponível através de replicação em multiplas Azs,
- Escala para massivos worloads e bancos de dados distribuidos,
- Chega em miliões de requisições por segundo, 
- Trilhões de linhas,
- 100s de tb de armazenamento,
- Rápido e consistente em performance, 
- Integrado com IAM,
- Permite programação dirigida por eventos com DynamoDb Streams,
- Baixo custo e capacidade de auto escala,

- Feito de tabelas,
- Cada tabela possui uma PK que deve ser decidida na hora da criação,
- Cada tabela pode ter infinitos numeros de linhas,
- Cada item possui atributos (são equivalentes a colunas- pode ser nulo),
- Cada item(linha) pode ter no máximo 400kb,
- Tipos de dados suportados:
  - String, num, binary, bool, null,
  - List, map,
  - Stringset, numberset, binaryset,

O exame vai testar 2 ver se vc sabe quais as 2 formas de definir PKS no dynamoDB,

Opção 1:

Partition keys(HASH):

- A partition key deve ser unica para da item(linha),
- A partition key deve ser unica, então o dado pode ser distribuído,
- Exemplo: user_id.

Opção 2:

Partition key + sort key(hash + range):

- A combinação das 2 deve ser única,
- Dado pe agrupado pela partition key,
- Exemplo user_id, game_id.


DynamoDb Read/Write Capacity mode / Vídeo 57:

Existem 2 modos básicos:

1 - Provisionado (padrão):
- Você especifica o número de leituras/escritas por segundo,
- Você precisa planejar com antecedência,
- Paga por unidades de escrita e leitura provisionada.

2 - On-demand:
- Leituras e escritas escalam automaticamente,
- Não precisa se planejar,
- Paga pelo uso mas mto mais caro.

Você pode alternar entre os modos a cada 24 hr.

RCU : Read capacity units,
WCU: Write capacity units

Fórmulas para calcular WCU e RCU,

1 WCU representa uma escrita por segundo para um item de 1 kb.






















































